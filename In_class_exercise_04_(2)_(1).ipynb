{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikiranreddymote/INF0_5731/blob/main/In_class_exercise_04_(2)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3yEAff05mB9"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBq2gkML5mCE"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuv-qvFH5mCF"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmGEhV6d5mCG"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ1iXu2S5mCJ"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "#load csv file and create dataframe\n",
        "df = pd.read_csv(r'C:\\Users\\admin\\Downloads\\Language Detection.csv (1)/Language Detection.csv')\n",
        "df\n",
        "\n",
        "#Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "#pip install gensim\n",
        "\n",
        "from gensim import corpora, models\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "K = 5\n",
        "#Tokenize the sentence into words\n",
        "\n",
        "\n",
        "#tokenazize the dataset\n",
        "df['tokenized'] = df['Text'].apply(lambda x: x.split())\n",
        "\n",
        "#create a dictionary of the tokens\n",
        "dictionary = df['tokenized'].apply(lambda x: list(set(x)))\n",
        "\n",
        "#dictionary = corpora.Dictionary(df['Text'].split())\n",
        "print(dictionary)\n",
        "corpus = dictionary.apply(lambda x: list(dictionary.index)).apply(lambda x: list(zip(x, [1]*len(x))))\n",
        "\n",
        "\n",
        "lda = models.LdaModel(corpus, num_topics=K, id2word=dictionary, passes=10)\n",
        "\n",
        "def print_topics(model, top_n=10):\n",
        "    for idx, topic in enumerate(model.top_topics(num_words=top_n)):\n",
        "        print('Topic {}: {}'.format(idx, ' '.join([word for word, prop in topic[1]])))\n",
        "print_topics(lda)\n",
        "#then summarize what are the topics\n",
        "lda.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2MThx1l5mCM"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQEeY0dA5mCN"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "#import libraries\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "#import LSA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim import corpora, models\n",
        "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score\n",
        "K = 5\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "lsa = TruncatedSVD(n_components=K, n_iter=10, random_state=42)\n",
        "#create a pipeline to normalize the tf-idf vectors\n",
        "pipeline = make_pipeline(TfidfVectorizer(), Normalizer(copy=False), lsa)\n",
        "#fit the pipeline to the data\n",
        "pipeline.fit(df['Text'])\n",
        "#summarize what are the topics\n",
        "print_topics(lsa)\n",
        "#then summarize what are the topics\n",
        "lsa.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UW1Wzc25mCO"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdDwTcTu5mCQ"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "#Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "# import ldavec\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.wrappers import LdaVec\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "K = 5\n",
        "#the number of iterations\n",
        "iterations = 10\n",
        "#the number of passes\n",
        "passes = 10\n",
        "lda2vec = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=passes, iterations=iterations)\n",
        "# summarize what are the topics\n",
        "print_topics(lda2vec)\n",
        "#then summarize what are the topics\n",
        "lda2vec.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SJ094I5mCR"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpjnrP665mCR"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "#Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "#import libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "# import BERTopic\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.wrappers import LdaVec\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "\n",
        "BERTopic = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=10, iterations=10)\n",
        "# summarize what are the topics\n",
        "print_topics(BERTopic)\n",
        "#then summarize what are the topics\n",
        "BERTopic.print_topics(num_topics=K, num_words=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beKtbUi45mCS"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35RZRGeg5mCT"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "# Compare the results generated by the four topic modeling algorithms\n",
        "\"\"\"\n",
        "The results are similar.\n",
        "\"\"\"\n",
        "#which is better between lda,lsa?\n",
        "\"\"\"\n",
        "LSA.\n",
        "\n",
        "in Natural Language Processing (NLP): Latent Semantic Analysis (LSA), \n",
        "Latent Dirichlet Allocation (LDA) and lexical chains. These techniques were evaluated \n",
        "and compared on two different corpora in order to highlight the similarities and differences \n",
        "between them from a semantic analysis viewpoint. The first corpus consisted of four Wikipedia \n",
        "articles on different topics, while the second one consisted of 35 online chat conversations \n",
        "between 4-12 participants debating four imposed topics (forum, chat, blog and wikis). The study \n",
        "focuses on finding similarities and differences between the outcomes of the three methods from a \n",
        "semantic analysis point of view, by computing quantitative factors such as correlations, degree of \n",
        "coverage of the resulting topics, etc. Using corpora from different types of discourse and quantitative \n",
        "factors that are task-independent allows us to prove that although LSA and LDA provide similar results,\n",
        "the results of lexical chaining are not very correlated with neither the ones of LSA or LDA, therefore \n",
        "lexical chains might be used complementary to LSA or LDA when performing semantic analysis for various NLP applications.\n",
        "These factors out to be the reason why the LSA are the better.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-aULoVo5mCV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
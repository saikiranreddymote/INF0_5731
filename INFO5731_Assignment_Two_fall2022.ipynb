{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikiranreddymote/INF0_5731/blob/main/INFO5731_Assignment_Two_fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "3b79deaa-bef1-4f5b-dbeb-39f4183fca62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n",
            "\u001b[K     |████████████████████████████████| 995 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 14.9 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[K     |████████████████████████████████| 384 kB 22.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.0.0rc9 h11-0.14.0 outcome-1.2.0 selenium-4.5.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.12 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [950 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,992 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,324 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,546 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,424 kB]\n",
            "Fetched 11.5 MB in 3s (3,417 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 91.7 MB of archives.\n",
            "After this operation, 309 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 105.0.5195.102-0ubuntu0.18.04.1 [1,156 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 105.0.5195.102-0ubuntu0.18.04.1 [80.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 105.0.5195.102-0ubuntu0.18.04.1 [5,097 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 105.0.5195.102-0ubuntu0.18.04.1 [5,320 kB]\n",
            "Fetched 91.7 MB in 4s (23.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_105.0.5195.102-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import requests, openpyxl\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!apt-get update\n",
        "from bs4 import BeautifulSoup as bs\n",
        "!apt install chromium-chromedriver\n",
        "import urllib.request as req\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import pandas as pd\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium import webdriver \n",
        "opt = webdriver.ChromeOptions()\n",
        "opt.add_argument('-headless')\n",
        "opt.add_argument('-no-sandbox')\n",
        "opt.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "JpF4pk7nrOOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "tits = [] \n",
        "revs= []\n",
        "link = 'https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3'\n",
        "driv = webdriver.Chrome('chromedriver',options=opt)\n",
        "driv.get(link)\n",
        "for num in range(43):         \n",
        "  driv.find_element(By.CLASS_NAME, \"ipl-load-more__button\").click()\n",
        "  time.sleep(5)\n",
        "  All_Tite = driv.find_elements(By.CLASS_NAME, \"title\")\n",
        "  All_Revs = driv.find_elements(By.CLASS_NAME, \"text\")\n",
        "  \n",
        "for e, r in zip(All_Tite, All_Revs):\n",
        "      tits.append((e.text).replace('\\n',''))\n",
        "      revs.append(r.text)\n",
        "      \n",
        "dat_taken = pd.DataFrame(list(zip(tits, revs)), columns =['Title_of_the_Review', 'Review_of_the_movie'])\n",
        "\n",
        "print(\"Length of data frame is\",len(dat_taken))\n",
        "print(dat_taken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwVGyN2SrQGE",
        "outputId": "c3057d11-6193-4c02-a7b4-d0e0605e24c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data frame is 1099\n",
            "                                    Title_of_the_Review  \\\n",
            "0                                Sort of disappointed..   \n",
            "1     Questionable script (and Awkwafina is just the...   \n",
            "2     Marks a bland start to the new phase of Marvel...   \n",
            "3     This could've been great if it wasn't for the ...   \n",
            "4     beautiful science fiction, fantasy and action ...   \n",
            "...                                                 ...   \n",
            "1094                             Could have been better   \n",
            "1095                                            Love it   \n",
            "1096                        Marvel does not disappoint.   \n",
            "1097                            Solid Story, Cgi was Eh   \n",
            "1098                                  Surprisingly good   \n",
            "\n",
            "                                    Review_of_the_movie  \n",
            "0     I got to admit I wasn't particularly blown awa...  \n",
            "1     The characters are a bit incoherent and straig...  \n",
            "2     Destin Daniel Cretton's 'Shang-Chi and the Leg...  \n",
            "3     The story was riddled with holes and plot issu...  \n",
            "4     Everything comes in it beautiful science ficti...  \n",
            "...                                                 ...  \n",
            "1094  Quite a lot of representation and it is great ...  \n",
            "1095                                                     \n",
            "1096                                                     \n",
            "1097  I was very impressed with the narrative of thi...  \n",
            "1098                                                     \n",
            "\n",
            "[1099 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vATjQNTY8buA",
        "outputId": "256d0b57-e612-432f-8832-2481eeec334b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('data')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsLiDOmDxsTD",
        "outputId": "bdea90f4-f705-4fb2-a34a-9439e06ee7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtf=dat_taken\n",
        "dtf=dtf[dtf[\"Review_of_the_movie\"]!= \"\"] #Removing null review\n",
        "print(dtf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBWXNDxIxyMg",
        "outputId": "ba96e4b6-815a-403c-82f1-171b9bae6e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Title_of_the_Review  \\\n",
            "0                                Sort of disappointed..   \n",
            "1     Questionable script (and Awkwafina is just the...   \n",
            "2     Marks a bland start to the new phase of Marvel...   \n",
            "3     This could've been great if it wasn't for the ...   \n",
            "4     beautiful science fiction, fantasy and action ...   \n",
            "...                                                 ...   \n",
            "1091                                        Great story   \n",
            "1092                                  A great adventure   \n",
            "1093                                       a nice movie   \n",
            "1094                             Could have been better   \n",
            "1097                            Solid Story, Cgi was Eh   \n",
            "\n",
            "                                    Review_of_the_movie  \n",
            "0     I got to admit I wasn't particularly blown awa...  \n",
            "1     The characters are a bit incoherent and straig...  \n",
            "2     Destin Daniel Cretton's 'Shang-Chi and the Leg...  \n",
            "3     The story was riddled with holes and plot issu...  \n",
            "4     Everything comes in it beautiful science ficti...  \n",
            "...                                                 ...  \n",
            "1091  Really original and shows a new direction the ...  \n",
            "1092  Shang-Chi and the Legend of the Ten Rings: The...  \n",
            "1093  I don't know why people giving bad reviews but...  \n",
            "1094  Quite a lot of representation and it is great ...  \n",
            "1097  I was very impressed with the narrative of thi...  \n",
            "\n",
            "[940 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "stopword_list = stopwords.words('english')\n",
        "snow_stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean(dtf):\n",
        "    dtf.loc[:,[\"clean_txt\"]]=dtf[\"Review_of_the_movie\"].apply(lambda ele: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", ele))\n",
        "    dtf.loc[:,[\"clean_txt\"]]=dtf[\"clean_txt\"].apply(lambda ele: re.sub(r\"\\d+\", \"\", ele))\n",
        "    dtf.loc[:,[\"clean_txt\"]]=dtf[\"clean_txt\"].apply(lambda string: ' '.join([w for w in string.split() if w not in (stopword_list)]))\n",
        "    dtf.loc[:,[\"clean_txt\"]]=dtf[\"clean_txt\"].str.lower()\n",
        "    dtf.loc[:,[\"tokens\"]] = dtf[\"clean_txt\"].apply(lambda x: word_tokenize(x))\n",
        "    def word_stemmer(text):\n",
        "      stem_text = [snow_stemmer.stem(i) for i in text]\n",
        "      return stem_text\n",
        "    dtf.loc[:,['text_stem']] = dtf[\"tokens\"].apply(lambda x: word_stemmer(x))\n",
        "    def word_lemmatizer(text):\n",
        "      lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
        "      return lem_text\n",
        "    dtf.loc[:,['text_lem']] = dtf[\"tokens\"].apply(lambda x: word_lemmatizer(x))\n",
        "    return dtf\n",
        "c = clean(dtf)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCrJ3Ci8zzqp",
        "outputId": "1b640b7f-882e-45f8-cfd2-d96b2430cd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(ilocs[0], value, pi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Title_of_the_Review  \\\n",
            "0                                Sort of disappointed..   \n",
            "1     Questionable script (and Awkwafina is just the...   \n",
            "2     Marks a bland start to the new phase of Marvel...   \n",
            "3     This could've been great if it wasn't for the ...   \n",
            "4     beautiful science fiction, fantasy and action ...   \n",
            "...                                                 ...   \n",
            "1091                                        Great story   \n",
            "1092                                  A great adventure   \n",
            "1093                                       a nice movie   \n",
            "1094                             Could have been better   \n",
            "1097                            Solid Story, Cgi was Eh   \n",
            "\n",
            "                                    Review_of_the_movie  \\\n",
            "0     I got to admit I wasn't particularly blown awa...   \n",
            "1     The characters are a bit incoherent and straig...   \n",
            "2     Destin Daniel Cretton's 'Shang-Chi and the Leg...   \n",
            "3     The story was riddled with holes and plot issu...   \n",
            "4     Everything comes in it beautiful science ficti...   \n",
            "...                                                 ...   \n",
            "1091  Really original and shows a new direction the ...   \n",
            "1092  Shang-Chi and the Legend of the Ten Rings: The...   \n",
            "1093  I don't know why people giving bad reviews but...   \n",
            "1094  Quite a lot of representation and it is great ...   \n",
            "1097  I was very impressed with the narrative of thi...   \n",
            "\n",
            "                                              clean_txt  \\\n",
            "0     i got admit i wasnt particularly blown away an...   \n",
            "1     the characters bit incoherent straight contrad...   \n",
            "2     destin daniel crettons shangchi legend ten rin...   \n",
            "3     the story riddled holes plot issues since awkw...   \n",
            "4     everything comes beautiful science fiction bea...   \n",
            "...                                                 ...   \n",
            "1091  really original shows new direction filming mc...   \n",
            "1092  shangchi legend ten rings the origin story sha...   \n",
            "1093  i dont know people giving bad reviews movie go...   \n",
            "1094  quite lot representation great trying expand m...   \n",
            "1097  i impressed narrative film not predictable ama...   \n",
            "\n",
            "                                                 tokens  \\\n",
            "0     [i, got, admit, i, wasnt, particularly, blown,...   \n",
            "1     [the, characters, bit, incoherent, straight, c...   \n",
            "2     [destin, daniel, crettons, shangchi, legend, t...   \n",
            "3     [the, story, riddled, holes, plot, issues, sin...   \n",
            "4     [everything, comes, beautiful, science, fictio...   \n",
            "...                                                 ...   \n",
            "1091  [really, original, shows, new, direction, film...   \n",
            "1092  [shangchi, legend, ten, rings, the, origin, st...   \n",
            "1093  [i, dont, know, people, giving, bad, reviews, ...   \n",
            "1094  [quite, lot, representation, great, trying, ex...   \n",
            "1097  [i, impressed, narrative, film, not, predictab...   \n",
            "\n",
            "                                              text_stem  \\\n",
            "0     [i, got, admit, i, wasnt, particular, blown, a...   \n",
            "1     [the, charact, bit, incoher, straight, contrad...   \n",
            "2     [destin, daniel, cretton, shangchi, legend, te...   \n",
            "3     [the, stori, riddl, hole, plot, issu, sinc, aw...   \n",
            "4     [everyth, come, beauti, scienc, fiction, beaut...   \n",
            "...                                                 ...   \n",
            "1091  [realli, origin, show, new, direct, film, mcu,...   \n",
            "1092  [shangchi, legend, ten, ring, the, origin, sto...   \n",
            "1093  [i, dont, know, peopl, give, bad, review, movi...   \n",
            "1094  [quit, lot, represent, great, tri, expand, mar...   \n",
            "1097  [i, impress, narrat, film, not, predict, amaz,...   \n",
            "\n",
            "                                               text_lem  \n",
            "0     [i, got, admit, i, wasnt, particularly, blown,...  \n",
            "1     [the, character, bit, incoherent, straight, co...  \n",
            "2     [destin, daniel, crettons, shangchi, legend, t...  \n",
            "3     [the, story, riddled, hole, plot, issue, since...  \n",
            "4     [everything, come, beautiful, science, fiction...  \n",
            "...                                                 ...  \n",
            "1091  [really, original, show, new, direction, filmi...  \n",
            "1092  [shangchi, legend, ten, ring, the, origin, sto...  \n",
            "1093  [i, dont, know, people, giving, bad, review, m...  \n",
            "1094  [quite, lot, representation, great, trying, ex...  \n",
            "1097  [i, impressed, narrative, film, not, predictab...  \n",
            "\n",
            "[940 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQKnPjPDHJHr",
        "outputId": "4dcc21c7-694c-4c07-9f93-1d498bce7059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.4.1)\n",
            "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (4.22.2)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.1.97)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.12.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.12.1+cu113)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.6.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (8.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.9->benepar) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.0.9->benepar) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.9->benepar) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (5.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading benepar_en2: Package 'benepar_en2' not found\n",
            "[nltk_data]     in index\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "%tensorflow_version 2.x\n",
        "!pip install benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "benepar.download('benepar_en3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def syn_struc(text):\n",
        "  for t in text:\n",
        "    for token in nlp(t):\n",
        "      return [token.text, '-',token.pos_,'-',token.tag_]\n",
        "dtf['pos']=dtf['tokens'].apply(lambda x: syn_struc(x))\n",
        "print(dtf['pos'].head())\n",
        "print(len(dtf['pos']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhl-fHmv03rg",
        "outputId": "330f8220-8320-4b1a-d3ff-8df36882f755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0            [i, -, PRON, -, PRP]\n",
            "1           [the, -, PRON, -, DT]\n",
            "2      [destin, -, PROPN, -, NNP]\n",
            "3           [the, -, PRON, -, DT]\n",
            "4    [everything, -, PRON, -, NN]\n",
            "Name: pos, dtype: object\n",
            "940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of extracting a sentence's dependencies in order to reflect its grammatical structure is known as dependency parsing.\n",
        "It explains how headwords and their dependents are related in terms of dependency.\n",
        "Dependency tags represent the relationship between two words in a sentence."
      ],
      "metadata": {
        "id": "dcIiZxxV1AUt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtLiqOLD8jL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constituency Parsing is the process of dissecting sentences into smaller units, usually referred to as constituents, for the purpose of analysis. These phrases fall within a certain grammar category, just as NPs (noun phrases) and VPs (verb phrase)."
      ],
      "metadata": {
        "id": "pHaZvV168m1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QI9wEtM88lix"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}